{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuFcDrCWJeDG",
        "colab_type": "text"
      },
      "source": [
        "# Final\n",
        "\n",
        "The final class project is to develop a model to detect pulmonary infection (pneumonia) on chest radiographs using any of the approaches and tools you have learned this quarter. The goal is both to create a high-performing algorithm for the target task, as well as to analyze performance across several different architecture permutations. At minimum, three different network designs of your choice will be tested (you are welcome to include more if you've tested others). As each model is built and trained, ensure to serialize the final model `*.hdf5` file before moving to the next iteration.\n",
        "\n",
        "This assignment is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uESKJtfgJeDH",
        "colab_type": "text"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Once complete, the following items must be submitted:\n",
        "\n",
        "* final `*.ipynb` notebook\n",
        "* final trained `*.hdf5` model files for **all** models (each independently saved)\n",
        "* final compiled `*.csv` file with performance statistics across the different architectures\n",
        "* final write-up with methods and results of experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "56d3oMiMw8Wm"
      },
      "source": [
        "# Google Colab\n",
        "\n",
        "The following lines of code will configure your Google Colab environment for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M6N7JTtJeDI",
        "colab_type": "text"
      },
      "source": [
        "### Enable GPU runtime\n",
        "\n",
        "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
        "\n",
        "```\n",
        "Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrFgUtWtJeDJ",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "The Google Colab environment is transient and will reset after any prolonged break in activity. To retain important and/or large files between sessions, use the following lines of code to mount your personal Google drive to this Colab instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUxyK4SXJeDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a6676ea-676c-4a10-f139-1b247253fce7"
      },
      "source": [
        "try:\n",
        "    # --- Mount gdrive to /content/drive/My Drive/\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "except: pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmcPNcOHJeDP",
        "colab_type": "text"
      },
      "source": [
        "Throughout this assignment we will use the following global `MOUNT_ROOT` variable to reference a location to store long-term data. If you are using a local Jupyter server and/or wish to store your data elsewhere, please update this variable now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vtG-93LJeDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Set data directory\n",
        "MOUNT_ROOT = '/content/drive/My Drive'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX9TMfS1JeDS",
        "colab_type": "text"
      },
      "source": [
        "### Select Tensorflow library version\n",
        "\n",
        "This assignment will use the (new) Tensorflow 2.1 library. Use the following line of code to select this updated version:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ7b4pVLJeDV",
        "colab_type": "text"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzHGWm_pJeDT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "outputId": "d756ca22-89dd-4140-89c6-d93e3eb4db43"
      },
      "source": [
        "# --- Select Tensorflow 2.0 (only in Google Colab)\n",
        "% tensorflow_version 2.x\n",
        "% pip install tensorflow-gpu==2.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.1 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (2.1.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.29.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.4.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (3.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (3.2.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (0.34.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (2.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (0.9.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (47.1.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.1) (2.10.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aKHOI3TJeDW",
        "colab_type": "text"
      },
      "source": [
        "### Jarvis library\n",
        "\n",
        "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X71bp26TJeDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "e432a082-d8be-4655-8afc-e4a5e8081b37"
      },
      "source": [
        "# --- Install jarvis (only in Google Colab or local runtime)\n",
        "% pip install jarvis-md"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jarvis-md in /usr/local/lib/python3.6/dist-packages (0.0.1a11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (1.0.4)\n",
            "Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (5.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (2.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->jarvis-md) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->jarvis-md) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->jarvis-md) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->jarvis-md) (2.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->jarvis-md) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->jarvis-md) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->jarvis-md) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->jarvis-md) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->jarvis-md) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->jarvis-md) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uakXuRWyJeDZ",
        "colab_type": "text"
      },
      "source": [
        "### Imports\n",
        "\n",
        "Use the following lines to import any additional needed libraries (note that depending on architecture choices, various additional modules will need to be specified here):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wshvSQqnJeDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, numpy as np, pandas as pd\n",
        "from tensorflow import losses, optimizers\n",
        "from tensorflow.keras import Input, Model, models, layers, metrics, regularizers\n",
        "from jarvis.train import datasets, custom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iCNfNWcJeDd",
        "colab_type": "text"
      },
      "source": [
        "# Data\n",
        "\n",
        "The data used in this tutorial will consist of (frontal projection) chest radiographs from the RSNA / Kaggle pneumonia challenge (https://www.kaggle.com/c/rsna-pneumonia-detection-challenge). The chest radiograph is the standard screening exam of choice to identify and trend changes in lung disease including infection (pneumonia). \n",
        "\n",
        "### Download\n",
        "\n",
        "The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/xr_pna`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4WZKdTpJeDe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f09f1517-5087-49fd-f58d-94107c8c6fce"
      },
      "source": [
        "# --- Download dataset\n",
        "datasets.download(name='xr/pna-crp')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'code': '/data/raw/xr_pna', 'data': '/data/raw/xr_pna'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnmh9ZU_JeDi",
        "colab_type": "text"
      },
      "source": [
        "### Python generators\n",
        "\n",
        "Once the dataset is downloaded locally, Python generators to iterate through the dataset can be easily prepared using the `datasets.prepare(...)` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJP6PoLWJeDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Prepare generators\n",
        "gen_train, gen_valid, client = datasets.prepare(name='xr/pna-crp', keyword='crp')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3dtf3McJeDl",
        "colab_type": "text"
      },
      "source": [
        "The created generators yield two variables for each iteration, `xs` and `ys`, each representing a dictionary of model input(s) and output(s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjA8qRuUJeDl",
        "colab_type": "text"
      },
      "source": [
        "### Model inputs\n",
        "\n",
        "For every input in `xs`, a corresponding `Input(...)` variable can be created and returned in a `inputs` dictionary for ease of model development:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ4oLoPLJeDm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5cf61ae4-6403-44cd-8ad7-99a449b47f0b"
      },
      "source": [
        "# --- Create model inputs\n",
        "inputs = client.get_inputs(Input)\n",
        "\n",
        "print(inputs.keys())\n",
        "print(inputs['dat'].shape)\n",
        "print(inputs['msk'].shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['dat', 'msk'])\n",
            "(None, 1, 256, 128, 1)\n",
            "(None, 1, 256, 128, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNIL6_0UJeDo",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "The goal of this project is to perform **global classification** for each image. In other words, regardless of algorithm choice, the final objective is to determine the absence or presence of pneumonia for each sample. This does **not** mean that you are required to use classification networks only; in fact it very well may be the case that a localization algorithm will overall perform better on this task.\n",
        "\n",
        "The task is designed to be open-ended on purpose. The only requirements are to:\n",
        "\n",
        "* test at minimum three different network architectures\n",
        "* one algorithm must use (at least) a classification type loss function\n",
        "* one algorithm must use (at least) a segmentation type loss function\n",
        "\n",
        "Note that the qualifier *at least* indicates that if you choose, you can use both classification and segmentation losses simultaneously for one algorithm to satisfy both requirements (this would allow you to test only non-classification / non-segmentation architectures for all remaining models if you choose)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfA-W_IIJ4_3",
        "colab_type": "text"
      },
      "source": [
        "#Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4CNY_2lKuee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Define kwargs dictionary\n",
        "kwargs = {\n",
        "    'kernel_size': (1, 3, 3),\n",
        "    'padding': 'same'}\n",
        "\n",
        "# --- Define lambda functions\n",
        "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.LeakyReLU()(x)\n",
        "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
        "\n",
        "# --- Define stride-1, stride-2 blocks\n",
        "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=(1, 2, 2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg4yIdRBKDnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Define model with extra initial layers\n",
        "l1 = conv1(32, conv1(32, inputs['dat']))\n",
        "l2 = conv1(48, conv1(48, conv2(48, l1)))\n",
        "l3 = conv1(64, conv1(64, conv2(64, l2)))\n",
        "l4 = conv1(80, conv1(80, conv2(80, l3)))\n",
        "l5 = conv1(96, conv1(96, conv2(96, l4)))\n",
        "l6 = conv1(112, conv1(112, conv2(112, l5)))\n",
        "\n",
        "# --- Flatten / reshape\n",
        "c1 = layers.Reshape((-1, 1, 1, 8 * 4 * 112))(l6)\n",
        "\n",
        "# --- Create logits\n",
        "logits = {}\n",
        "logits['pna-cls'] = layers.Conv3D(filters=2, kernel_size=(1, 1, 1), name='pna-cls')(c1)\n",
        "\n",
        "# --- Create model\n",
        "model_cls = Model(inputs=inputs, outputs=logits) \n",
        "\n",
        "# --- Compile the model\n",
        "model_cls.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
        "    loss={'pna-cls': losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
        "    metrics={'pna-cls': metrics.SparseCategoricalAccuracy()},\n",
        "    experimental_run_tf_function=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzGZ_iVBLRP-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "cb1fd82d-52cc-4b7f-fcca-823eb63bf983"
      },
      "source": [
        "# --- Load data into memory for faster training\n",
        "client.load_data_in_memory()\n",
        "\n",
        "# --- Train the model\n",
        "model_cls.fit(\n",
        "    x=gen_train, \n",
        "    steps_per_epoch=250, \n",
        "    epochs=12,\n",
        "    validation_data=gen_valid,\n",
        "    validation_steps=250,\n",
        "    validation_freq=4)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2020-06-11 10:31:58 ] [====================] 100.000% : Iterating | 029694    Epoch 1/12\n",
            "250/250 [==============================] - 44s 177ms/step - loss: 0.5312 - sparse_categorical_accuracy: 0.7870\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 32s 130ms/step - loss: 0.4454 - sparse_categorical_accuracy: 0.8203\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 32s 129ms/step - loss: 0.3896 - sparse_categorical_accuracy: 0.8287\n",
            "Epoch 4/12\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3924 - sparse_categorical_accuracy: 0.8327Epoch 1/12\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.4285 - sparse_categorical_accuracy: 0.8273\n",
            "250/250 [==============================] - 45s 179ms/step - loss: 0.3933 - sparse_categorical_accuracy: 0.8320 - val_loss: 0.4285 - val_sparse_categorical_accuracy: 0.8273\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 32s 129ms/step - loss: 0.3860 - sparse_categorical_accuracy: 0.8393\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 32s 129ms/step - loss: 0.3550 - sparse_categorical_accuracy: 0.8493\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 32s 129ms/step - loss: 0.3457 - sparse_categorical_accuracy: 0.8463\n",
            "Epoch 8/12\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3511 - sparse_categorical_accuracy: 0.8507Epoch 1/12\n",
            "250/250 [==============================] - 11s 45ms/step - loss: 0.6403 - sparse_categorical_accuracy: 0.6470\n",
            "250/250 [==============================] - 44s 174ms/step - loss: 0.3518 - sparse_categorical_accuracy: 0.8507 - val_loss: 0.6403 - val_sparse_categorical_accuracy: 0.6470\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 32s 129ms/step - loss: 0.3375 - sparse_categorical_accuracy: 0.8613\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 32s 129ms/step - loss: 0.3487 - sparse_categorical_accuracy: 0.8557\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 32s 129ms/step - loss: 0.3457 - sparse_categorical_accuracy: 0.8613\n",
            "Epoch 12/12\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.3382 - sparse_categorical_accuracy: 0.8648Epoch 1/12\n",
            "250/250 [==============================] - 11s 45ms/step - loss: 0.3605 - sparse_categorical_accuracy: 0.8500\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 0.3385 - sparse_categorical_accuracy: 0.8643 - val_loss: 0.3605 - val_sparse_categorical_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc401bce7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWLFPK1aLiX3",
        "colab_type": "text"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHQVrEBGZbBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname = '{}/models/final/model_cls.hdf5'.format(MOUNT_ROOT)\n",
        "model_cls = models.load_model(fname, compile=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ilCT1ieLVeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_stats(pred, true):\n",
        "    \"\"\"\n",
        "    Method to calculate binary classification stats\n",
        "    \n",
        "    \"\"\"\n",
        "    # --- Calculate true positives / true negatives\n",
        "    tp = np.count_nonzero((true == 1) & (pred == 1))\n",
        "    tn = np.count_nonzero((true == 0) & (pred == 0))\n",
        "    fp = np.count_nonzero((true == 0) & (pred == 1))\n",
        "    fn = np.count_nonzero((true == 1) & (pred == 0))\n",
        "\n",
        "    # --- Stats\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "    sen = tp / (tp + fn)\n",
        "    spe = tn / (tn + fp)\n",
        "    ppv = tp / (tp + fp)\n",
        "    npv = tn / (tn + fn)\n",
        "    \n",
        "    return acc, sen, spe, ppv, npv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y4qlMvxMLu9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9df82536-10a8-493c-a9ec-0a0f68ddc33b"
      },
      "source": [
        "# --- Run model prediction for segmentation masks\n",
        "test_train, test_valid = client.create_generators(test=True)\n",
        "\n",
        "pred = []\n",
        "true = []\n",
        "\n",
        "for xs, ys in test_train:\n",
        "    \n",
        "    logs = model_cls.predict(xs)\n",
        "    cls_ = np.argmax(logs, axis=-1)\n",
        "    \n",
        "    # --- Record number of pixels in mask\n",
        "    pred.append(cls_.squeeze())\n",
        "    true.append(ys['pna-cls'].squeeze())\n",
        "\n",
        "    if len(pred) == 1000:\n",
        "        break\n",
        "\n",
        "# --- Convert to arrays\n",
        "cls_tr_pred = np.array(pred)\n",
        "cls_tr_true = np.array(true)\n",
        "cls_tr_mean = calculate_stats(cls_tr_pred, cls_tr_true)[0]\n",
        "print (cls_tr_mean)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2020-06-11 15:22:36 ] [>...................] 4.200% : Iterating | 001000      0.876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StYLW8naMf_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73eadb73-31e0-4c81-8a74-fb4654b97ed8"
      },
      "source": [
        "pred = []\n",
        "true = []\n",
        "\n",
        "for xs, ys in test_valid:\n",
        "    \n",
        "    logs = model_cls.predict(xs)\n",
        "    cls_ = np.argmax(logs, axis=-1)\n",
        "    \n",
        "    # --- Record number of pixels in mask\n",
        "    pred.append(cls_.squeeze())\n",
        "    true.append(ys['pna-cls'].squeeze())\n",
        "\n",
        "    if len(pred) == 1000:\n",
        "        break\n",
        "\n",
        "# --- Convert to arrays\n",
        "cls_val_pred = np.array(pred)\n",
        "cls_val_true = np.array(true)\n",
        "cls_val_mean = calculate_stats(cls_val_pred, cls_val_true)[0]\n",
        "print (cls_val_mean)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2020-06-11 15:23:13 ] [===>................] 16.992% : Iterating | 001000     0.863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN-VgtA6J43j",
        "colab_type": "text"
      },
      "source": [
        "#Segmentation Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXk2wKS5LfA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Select shape\n",
        "configs = {'specs': {'xs': {'dat': {'shape': [3, 256, 128, 1]}}}, \n",
        "           'batch': {'size': 12}}\n",
        "\n",
        "# --- Cropped 256 x 128 (multiple step)\n",
        "gen_train, gen_valid, client = datasets.prepare(name='xr/pna-crp', keyword='crp', configs=configs)\n",
        "inputs = client.get_inputs(Input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRbs0Sm9NgdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Define 2D conv (xy-features)\n",
        "conv_2d = lambda x, filters, strides : layers.Conv3D(\n",
        "    filters=filters, \n",
        "    strides=strides, \n",
        "    kernel_size=(1, 3, 3), \n",
        "    padding='same',\n",
        "    kernel_initializer='he_normal')(x)\n",
        "\n",
        "# --- Define 1D conv (z-features)\n",
        "conv_1d = lambda x, filters, k=2 : layers.Conv3D(\n",
        "    filters=filters,\n",
        "    strides=1,\n",
        "    kernel_size=(k, 1, 1),\n",
        "    padding='valid',\n",
        "    kernel_initializer='he_normal')(x)\n",
        "\n",
        "# --- Define stride-1 3D, stride-2 3D and stride-1 1D (z-subsample) blocks\n",
        "conv1 = lambda filters, x : relu(norm(conv_2d(x, filters, strides=(1, 1, 1))))\n",
        "conv2 = lambda filters, x : relu(norm(conv_2d(x, filters, strides=(1, 2, 2))))\n",
        "convZ = lambda filters, k, x : relu(norm(conv_1d(x, filters, k=k)))\n",
        "\n",
        "# --- Define 2D transpose\n",
        "tran = lambda x, filters : layers.Conv3DTranspose(\n",
        "    filters=filters, \n",
        "    strides=(1, 2, 2),\n",
        "    kernel_size=(1, 3, 3),\n",
        "    padding='same',\n",
        "    kernel_initializer='he_normal')(x)\n",
        "\n",
        "# --- Define transpose block\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I1HTWtoNg-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Define model\n",
        "l1 = conv1(32,  inputs['dat'])\n",
        "l2 = conv1(48, conv2(48, l1))\n",
        "l3 = conv1(64, conv2(64, l2))\n",
        "l4 = conv1(80, convZ(80, 2, conv2(80, l3)))\n",
        "l5 = conv1(96, convZ(96, 2, conv2(96, l4)))\n",
        "l6 =  tran2(80, conv1(80, l5))\n",
        "l7 =  tran2(64, conv1(80, convZ(80, 2, l4) + l6))\n",
        "l8 =  tran2(48, conv1(64, convZ(64, 3, l3) + l7))\n",
        "l9 =  tran2(32,  conv1(48, convZ(48, 3, l2) + l8))\n",
        "l10 = conv1(32,  conv1(32,  convZ(32, 3, l1) + l9))\n",
        "\n",
        "\n",
        "# --- Create logits\n",
        "logits_custom = {}\n",
        "logits_custom['pna-seg'] = layers.Conv3D(\n",
        "    name='pna-seg',\n",
        "    filters=3, \n",
        "    strides=1, \n",
        "    kernel_size=(1, 3, 3), \n",
        "    padding='same',\n",
        "    kernel_initializer='he_normal')(l10)\n",
        "\n",
        "# --- Create model\n",
        "model_seg = Model(inputs=inputs, outputs=logits_custom)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "871YDP6MORfa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "88f53b22-dc1f-40b7-b713-da0b67c3e175"
      },
      "source": [
        "# --- Create metrics\n",
        "metrics = custom.dsc(weights=inputs['msk'])\n",
        "metrics += [custom.softmax_ce_sens(weights=inputs['msk'])]\n",
        "metrics = {'pna-seg': metrics}\n",
        "\n",
        "# --- Compile the model\n",
        "model_seg.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
        "    loss={'pna-seg': custom.sce(inputs['msk'])},\n",
        "    metrics=metrics,\n",
        "    experimental_run_tf_function=False)\n",
        "\n",
        "# --- Load data into memory for faster training\n",
        "client.load_data_in_memory()\n",
        "\n",
        "# --- Train the model\n",
        "model_seg.fit(\n",
        "    x=gen_train, \n",
        "    steps_per_epoch=250, \n",
        "    epochs=12,\n",
        "    validation_data=gen_valid,\n",
        "    validation_steps=250,\n",
        "    validation_freq=4)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2020-06-11 11:59:55 ] [====================] 100.000% : Iterating | 029694    Epoch 1/12\n",
            "250/250 [==============================] - 113s 451ms/step - loss: 0.1261 - dsc_1: 0.4012 - softmax_ce_sens: 0.3145\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 106s 422ms/step - loss: 0.0964 - dsc_1: 0.5153 - softmax_ce_sens: 0.4696\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 105s 422ms/step - loss: 0.0905 - dsc_1: 0.5442 - softmax_ce_sens: 0.5018\n",
            "Epoch 4/12\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.0919 - dsc_1: 0.5451 - softmax_ce_sens: 0.5042Epoch 1/12\n",
            "250/250 [==============================] - 36s 146ms/step - loss: 0.0957 - dsc_1: 0.5121 - softmax_ce_sens: 0.4577\n",
            "250/250 [==============================] - 142s 568ms/step - loss: 0.0922 - dsc_1: 0.5442 - softmax_ce_sens: 0.5030 - val_loss: 0.0957 - val_dsc_1: 0.5121 - val_softmax_ce_sens: 0.4577\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 106s 422ms/step - loss: 0.0833 - dsc_1: 0.5527 - softmax_ce_sens: 0.5181\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 106s 423ms/step - loss: 0.0851 - dsc_1: 0.5970 - softmax_ce_sens: 0.5619\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 106s 422ms/step - loss: 0.0853 - dsc_1: 0.5815 - softmax_ce_sens: 0.5530\n",
            "Epoch 8/12\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.0890 - dsc_1: 0.5814 - softmax_ce_sens: 0.5489Epoch 1/12\n",
            "250/250 [==============================] - 35s 140ms/step - loss: 0.0918 - dsc_1: 0.5817 - softmax_ce_sens: 0.5660\n",
            "250/250 [==============================] - 141s 562ms/step - loss: 0.0890 - dsc_1: 0.5819 - softmax_ce_sens: 0.5491 - val_loss: 0.0918 - val_dsc_1: 0.5817 - val_softmax_ce_sens: 0.5660\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 106s 422ms/step - loss: 0.0861 - dsc_1: 0.5886 - softmax_ce_sens: 0.5568\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 106s 422ms/step - loss: 0.0866 - dsc_1: 0.6000 - softmax_ce_sens: 0.5682\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 106s 422ms/step - loss: 0.0831 - dsc_1: 0.5853 - softmax_ce_sens: 0.5554\n",
            "Epoch 12/12\n",
            "249/250 [============================>.] - ETA: 0s - loss: 0.0822 - dsc_1: 0.6119 - softmax_ce_sens: 0.5873Epoch 1/12\n",
            "250/250 [==============================] - 34s 138ms/step - loss: 0.0875 - dsc_1: 0.5855 - softmax_ce_sens: 0.5316\n",
            "250/250 [==============================] - 140s 559ms/step - loss: 0.0822 - dsc_1: 0.6119 - softmax_ce_sens: 0.5871 - val_loss: 0.0875 - val_dsc_1: 0.5855 - val_softmax_ce_sens: 0.5316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc3fc8f59b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tMwv_PyOelL",
        "colab_type": "text"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSN1TA47WjGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname = '{}/models/final/model_seg.hdf5'.format(MOUNT_ROOT)\n",
        "model_seg = models.load_model(fname, compile=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqXvGq2OlYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_stats(nnzs, true, threshold):\n",
        "    \"\"\"\n",
        "    Method to calculate stats at given threshold of mask pixels\n",
        "    \n",
        "    \"\"\"\n",
        "    # --- Calculate true positives / true negatives\n",
        "    tp = np.count_nonzero(nnzs[true == 1] >= threshold)\n",
        "    tn = np.count_nonzero(nnzs[true == 0] < threshold)\n",
        "    fp = np.count_nonzero(nnzs[true == 0] >= threshold)\n",
        "    fn = np.count_nonzero(nnzs[true == 1] < threshold)\n",
        "\n",
        "    # --- Stats\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "    sen = tp / (tp + fn)\n",
        "    spe = tn / (tn + fp)\n",
        "    ppv = tp / (tp + fp)\n",
        "    npv = tn / (tn + fn)\n",
        "    \n",
        "    return acc, sen, spe, ppv, npv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKifSMqqOfuU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44208eb4-0ed2-4680-981d-3ba95e82f413"
      },
      "source": [
        "# --- Run model prediction for segmentation masks\n",
        "test_train, test_valid = client.create_generators(test=True)\n",
        "\n",
        "nnzs = []\n",
        "true = []\n",
        "\n",
        "for xs, ys in test_train:\n",
        "    \n",
        "    logs = model_seg.predict(xs)\n",
        "    pred = np.argmax(logs, axis=-1)\n",
        "    \n",
        "    # --- Remove masked pixels if needed\n",
        "    pred[xs['msk'][..., 0] == 0] = 0\n",
        "    \n",
        "    # --- Record number of pixels in mask\n",
        "    nnzs.append(np.count_nonzero(pred))\n",
        "    true.append(ys['pna-cls'].squeeze())\n",
        "    \n",
        "    # --- Break after 1000 exams\n",
        "    if len(nnzs) == 1000:\n",
        "        break\n",
        "\n",
        "# --- Convert to arrays\n",
        "seg_tr_nnzs = np.array(nnzs)\n",
        "seg_tr_true = np.array(true)\n",
        "seg_tr_mean = calculate_stats(seg_tr_nnzs, seg_tr_true, 1000)[0]\n",
        "print (seg_tr_mean)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2020-06-11 15:23:59 ] [>...................] 4.200% : Iterating | 001000      "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyk4LedhOoRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecaa868b-ba79-4059-da65-186eafa8a575"
      },
      "source": [
        "nnzs = []\n",
        "true = []\n",
        "\n",
        "for xs, ys in test_valid:\n",
        "    \n",
        "    logs = model_seg.predict(xs)\n",
        "    pred = np.argmax(logs, axis=-1)\n",
        "    \n",
        "    # --- Remove masked pixels if needed\n",
        "    pred[xs['msk'][..., 0] == 0] = 0\n",
        "    \n",
        "    # --- Record number of pixels in mask\n",
        "    nnzs.append(np.count_nonzero(pred))\n",
        "    true.append(ys['pna-cls'].squeeze())\n",
        "    \n",
        "    # --- Break after 1000 exams\n",
        "    if len(nnzs) == 1000:\n",
        "        break\n",
        "\n",
        "# --- Convert to arrays\n",
        "seg_val_nnzs = np.array(nnzs)\n",
        "seg_val_true = np.array(true)\n",
        "seg_val_mean = calculate_stats(seg_val_nnzs, seg_val_true, 1000)[0]\n",
        "print (seg_val_mean)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2020-06-11 15:24:41 ] [===>................] 16.992% : Iterating | 001000     0.884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMPxU0ACJ4gm",
        "colab_type": "text"
      },
      "source": [
        "#Dual loss function Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62tNPRw0g4Gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jarvis.utils.general import overload, tools as jtools\n",
        "from jarvis.train.client import Client\n",
        "\n",
        "@overload(Client)\n",
        "def preprocess(self, arrays, **kwargs):\n",
        "    \"\"\"\n",
        "    Method to create a custom msk array for class weights and/or masks\n",
        "    \n",
        "    \"\"\"\n",
        "    # --- Create msk\n",
        "    msk = np.zeros(arrays['xs']['dat'].shape)\n",
        "\n",
        "    lng = arrays['xs']['msk']\n",
        "    pna = arrays['ys']['pna-seg']\n",
        "\n",
        "    msk[lng > 0] = 1\n",
        "    msk[pna > 0] = 1\n",
        "\n",
        "    arrays['xs']['msk'] = msk\n",
        "    \n",
        "    return arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVKNqiOqhPah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Prepare generators\n",
        "configs = {'batch': {'size': 8}}\n",
        "gen_train, gen_valid, client = datasets.prepare(name='xr/pna-crp', keyword='crp', configs=configs)\n",
        "\n",
        "# --- Manually create generators\n",
        "gen_train, gen_valid = client.create_generators()\n",
        "\n",
        "# --- Create model inputs\n",
        "inputs = client.get_inputs(Input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBFZQ1NshReu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Define kwargs dictionary\n",
        "kwargs = {\n",
        "    'kernel_size': (1, 3, 3),\n",
        "    'padding': 'same',\n",
        "    'kernel_regularizer': regularizers.l2(0.01)}\n",
        "\n",
        "# --- Define lambda functions\n",
        "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.LeakyReLU()(x)\n",
        "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
        "\n",
        "# --- Define stride-1, stride-2 blocks\n",
        "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=(1, 2, 2))))\n",
        "\n",
        "# --- Define dropout\n",
        "drop = layers.Dropout(rate=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_JOqG6phS7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Define contracting layers\n",
        "l1 = conv1(8, inputs['dat'])\n",
        "l2 = conv1(16, conv2(16, l1))\n",
        "l3 = conv1(32, conv2(32, l2))\n",
        "l4 = conv1(48, conv2(48, l3))\n",
        "l5 = conv1(64, conv2(64, l4))\n",
        "l6 = conv1(80, conv2(80, l5))\n",
        "l6_conv = drop(conv1(96, conv2(96, l6)))\n",
        "\n",
        "\n",
        "# --- Define expanding layers\n",
        "l6_tran = tran2(80, l6_conv)\n",
        "l7  = tran2(64, conv1(80, l6_tran  + l6))\n",
        "l8  = tran2(48, conv1(64, l7  + l5))\n",
        "l9  = tran2(32, conv1(48, l8  + l4))\n",
        "l10 = tran2(16, conv1(32, l9  + l3))\n",
        "l11 = tran2(8,  conv1(16, l10 + l2))\n",
        "l12 = conv1(8,  conv1(8,  l11 + l1))\n",
        "\n",
        "# --- Create classifier feature vector\n",
        "c1 = layers.Reshape((-1, 1, 1, 4 * 2 * 96))(l6_conv)\n",
        "\n",
        "# --- Create logits\n",
        "logits = {}\n",
        "logits['pna-seg'] = layers.Conv3D(filters=2, name='pna-seg', **kwargs)(l12)\n",
        "logits['pna-cls'] = layers.Conv3D(filters=2, kernel_size=(1, 1, 1), name='pna-cls')(c1)\n",
        "\n",
        "# --- Create model\n",
        "model_duo = Model(inputs=inputs, outputs=logits) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EhTfKZ5hn-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Compile the model\n",
        "model_duo.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
        "    loss={\n",
        "        'pna-seg': custom.sce(inputs['msk']),\n",
        "        'pna-cls': losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
        "    metrics={\n",
        "        'pna-seg': custom.dsc(weights=inputs['msk']),\n",
        "        'pna-cls': metrics.SparseCategoricalAccuracy()\n",
        "        },\n",
        "    experimental_run_tf_function=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swSLiciwjQ7U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "30152975-56e7-4d09-c73a-0513f1d10309"
      },
      "source": [
        "# --- Load data into memory for faster training\n",
        "client.load_data_in_memory()\n",
        "\n",
        "# --- Train model\n",
        "model_duo.fit(\n",
        "    x=gen_train, \n",
        "    steps_per_epoch=100, \n",
        "    epochs=12,\n",
        "    validation_data=gen_valid,\n",
        "    validation_steps=100,\n",
        "    validation_freq=4,\n",
        "    use_multiprocessing=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2020-06-11 14:19:59 ] [====================] 100.000% : Iterating | 029694    WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/12\n",
            "100/100 [==============================] - 15s 153ms/step - loss: 10.8002 - pna-cls_loss: 0.6255 - pna-seg_loss: 0.2264 - pna-cls_sparse_categorical_accuracy: 0.7387 - pna-seg_dsc_1: 0.3752\n",
            "Epoch 2/12\n",
            "100/100 [==============================] - 7s 69ms/step - loss: 8.5053 - pna-cls_loss: 0.5832 - pna-seg_loss: 0.1499 - pna-cls_sparse_categorical_accuracy: 0.7600 - pna-seg_dsc_1: 0.4143\n",
            "Epoch 3/12\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 7.4186 - pna-cls_loss: 0.5325 - pna-seg_loss: 0.1251 - pna-cls_sparse_categorical_accuracy: 0.7700 - pna-seg_dsc_1: 0.5264\n",
            "Epoch 4/12\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 6.8221 - pna-cls_loss: 0.4753 - pna-seg_loss: 0.1207 - pna-cls_sparse_categorical_accuracy: 0.8018 - pna-seg_dsc_1: 0.5952WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/12\n",
            "100/100 [==============================] - 5s 48ms/step - loss: 6.9043 - pna-cls_loss: 0.6710 - pna-seg_loss: 0.2061 - pna-cls_sparse_categorical_accuracy: 0.7462 - pna-seg_dsc_1: 0.3798\n",
            "100/100 [==============================] - 12s 117ms/step - loss: 6.8173 - pna-cls_loss: 0.4725 - pna-seg_loss: 0.1207 - pna-cls_sparse_categorical_accuracy: 0.8037 - pna-seg_dsc_1: 0.5932 - val_loss: 6.9043 - val_pna-cls_loss: 0.6710 - val_pna-seg_loss: 0.2061 - val_pna-cls_sparse_categorical_accuracy: 0.7462 - val_pna-seg_dsc_1: 0.3798\n",
            "Epoch 5/12\n",
            "100/100 [==============================] - 7s 69ms/step - loss: 6.4290 - pna-cls_loss: 0.4411 - pna-seg_loss: 0.1149 - pna-cls_sparse_categorical_accuracy: 0.8163 - pna-seg_dsc_1: 0.6184\n",
            "Epoch 6/12\n",
            "100/100 [==============================] - 7s 68ms/step - loss: 6.1436 - pna-cls_loss: 0.4295 - pna-seg_loss: 0.1120 - pna-cls_sparse_categorical_accuracy: 0.8200 - pna-seg_dsc_1: 0.6213\n",
            "Epoch 7/12\n",
            "100/100 [==============================] - 7s 67ms/step - loss: 5.9273 - pna-cls_loss: 0.4468 - pna-seg_loss: 0.1142 - pna-cls_sparse_categorical_accuracy: 0.8037 - pna-seg_dsc_1: 0.5620\n",
            "Epoch 8/12\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 5.7030 - pna-cls_loss: 0.4443 - pna-seg_loss: 0.1107 - pna-cls_sparse_categorical_accuracy: 0.8194 - pna-seg_dsc_1: 0.5889WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/12\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 5.6519 - pna-cls_loss: 0.4781 - pna-seg_loss: 0.1344 - pna-cls_sparse_categorical_accuracy: 0.7887 - pna-seg_dsc_1: 0.5320\n",
            "100/100 [==============================] - 10s 98ms/step - loss: 5.7010 - pna-cls_loss: 0.4428 - pna-seg_loss: 0.1112 - pna-cls_sparse_categorical_accuracy: 0.8200 - pna-seg_dsc_1: 0.5881 - val_loss: 5.6519 - val_pna-cls_loss: 0.4781 - val_pna-seg_loss: 0.1344 - val_pna-cls_sparse_categorical_accuracy: 0.7887 - val_pna-seg_dsc_1: 0.5320\n",
            "Epoch 9/12\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 5.4456 - pna-cls_loss: 0.4002 - pna-seg_loss: 0.1096 - pna-cls_sparse_categorical_accuracy: 0.8313 - pna-seg_dsc_1: 0.6195\n",
            "Epoch 10/12\n",
            "100/100 [==============================] - 7s 69ms/step - loss: 5.2712 - pna-cls_loss: 0.4220 - pna-seg_loss: 0.1200 - pna-cls_sparse_categorical_accuracy: 0.8175 - pna-seg_dsc_1: 0.6149\n",
            "Epoch 11/12\n",
            "100/100 [==============================] - 7s 69ms/step - loss: 5.0919 - pna-cls_loss: 0.4570 - pna-seg_loss: 0.1074 - pna-cls_sparse_categorical_accuracy: 0.7962 - pna-seg_dsc_1: 0.6084\n",
            "Epoch 12/12\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.8799 - pna-cls_loss: 0.4473 - pna-seg_loss: 0.1028 - pna-cls_sparse_categorical_accuracy: 0.7992 - pna-seg_dsc_1: 0.5629WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/12\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 4.8349 - pna-cls_loss: 0.4713 - pna-seg_loss: 0.1341 - pna-cls_sparse_categorical_accuracy: 0.8037 - pna-seg_dsc_1: 0.5968\n",
            "100/100 [==============================] - 10s 100ms/step - loss: 4.8765 - pna-cls_loss: 0.4449 - pna-seg_loss: 0.1028 - pna-cls_sparse_categorical_accuracy: 0.8000 - pna-seg_dsc_1: 0.5638 - val_loss: 4.8349 - val_pna-cls_loss: 0.4713 - val_pna-seg_loss: 0.1341 - val_pna-cls_sparse_categorical_accuracy: 0.8037 - val_pna-seg_dsc_1: 0.5968\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f071daade80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCDsRPIePdoc",
        "colab_type": "text"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdt_YtP84Qrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname = '{}/models/final/model_duo.hdf5'.format(MOUNT_ROOT)\n",
        "model_duo = models.load_model(fname, compile=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8WmojzArS-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_stats(nnzs, true, threshold):\n",
        "    \"\"\"\n",
        "    Method to calculate stats at given threshold of mask pixels\n",
        "    \n",
        "    \"\"\"\n",
        "    # --- Calculate true positives / true negatives\n",
        "    tp = np.count_nonzero(nnzs[true == 1] >= threshold)\n",
        "    tn = np.count_nonzero(nnzs[true == 0] < threshold)\n",
        "    fp = np.count_nonzero(nnzs[true == 0] >= threshold)\n",
        "    fn = np.count_nonzero(nnzs[true == 1] < threshold)\n",
        "\n",
        "    # --- Stats\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "    sen = tp / (tp + fn)\n",
        "    spe = tn / (tn + fp)\n",
        "    ppv = tp / (tp + fp)\n",
        "    npv = tn / (tn + fn)\n",
        "    \n",
        "    return acc, sen, spe, ppv, npv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YOufeamrVhL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a8c8058-eb6b-46ea-eac4-fabb0e4ecbfa"
      },
      "source": [
        "# --- Run model prediction for segmentation masks\n",
        "test_train, test_valid = client.create_generators(test=True)\n",
        "\n",
        "nnzs = []\n",
        "true = []\n",
        "\n",
        "for xs, ys in test_train:\n",
        "    \n",
        "    logits = model_duo.predict(xs)\n",
        "    pred = np.argmax(logits[1], axis=-1)\n",
        "     \n",
        "    # --- Remove masked pixels if needed\n",
        "    pred[xs['msk'][..., 0] == 0] = 0\n",
        "    \n",
        "    # --- Record number of pixels in mask\n",
        "    nnzs.append(np.count_nonzero(pred))\n",
        "    true.append(ys['pna-cls'].squeeze())\n",
        "    \n",
        "    # --- Break after 1000 exams\n",
        "    if len(nnzs) == 1000:\n",
        "        break\n",
        "\n",
        "duo_tr_nnzs = np.array(nnzs)\n",
        "duo_tr_true = np.array(true)\n",
        "\n",
        "duo_tr_mean = calculate_stats(duo_tr_nnzs, duo_tr_true, 1000)[0]\n",
        "print (duo_tr_mean)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2020-06-11 15:25:21 ] [>...................] 4.200% : Iterating | 001000      0.867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0QX-KGCs0fI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "778ab8a3-ee16-453b-cb0a-14ca74136b89"
      },
      "source": [
        "nnzs = []\n",
        "true = []\n",
        "\n",
        "for xs, ys in test_valid:\n",
        "    \n",
        "    logits = model_duo.predict(xs)\n",
        "    pred = np.argmax(logits[1], axis=-1)\n",
        "    \n",
        "    # --- Remove masked pixels if needed\n",
        "    pred[xs['msk'][..., 0] == 0] = 0\n",
        "    \n",
        "    # --- Record number of pixels in mask\n",
        "    nnzs.append(np.count_nonzero(pred))\n",
        "    true.append(ys['pna-cls'].squeeze())\n",
        "    \n",
        "    # --- Break after 1000 exams\n",
        "    if len(nnzs) == 1000:\n",
        "        break\n",
        "\n",
        "duo_val_nnzs = np.array(nnzs)\n",
        "duo_val_true = np.array(true)\n",
        "\n",
        "duo_val_mean = calculate_stats(duo_val_nnzs, duo_val_true, 1000)[0]\n",
        "print (duo_val_mean)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2020-06-11 15:26:01 ] [===>................] 16.992% : Iterating | 001000     0.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A16kNhfTJeDt",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "For each of the three models, the following metrics should be calculated for **both the training and validation** cohorts:\n",
        "\n",
        "* overall accuracy (mean)\n",
        "* overall sensitivity\n",
        "* overall specificity\n",
        "* overall positive predictive value (PPV)\n",
        "* overall negative predictive value (NPV)\n",
        "\n",
        "### Performance\n",
        "\n",
        "The only requirement for full credit is that your overall top-performing model achieves an image-by-image accuracy rate of 0.85 or above. In addition, the **top three performing models** out of the entire class will recieve a full letter bonus to your overall final grade (e.g. C to B, B to A, etc). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VS_JOjmJeDw",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "\n",
        "When ready, create a `*.csv` file with your compiled **training and validation** cohort statistics for the different models. Consider the following table format (although any format that contains the required information is sufficient):\n",
        "\n",
        "```\n",
        "          ACCURACY                    \n",
        "          mean | median | 25th-tile | 75th-tile \n",
        "model 1\n",
        "model 2\n",
        "model 3\n",
        "...\n",
        "```\n",
        "\n",
        "As above, tables for both training and validation should be provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqTSuqs0JeDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# --- Create *.csv\n",
        "data_train = {'tr_mean':[cls_tr_mean, seg_tr_mean, duo_tr_mean],\n",
        "         'tr_median':[np.median(cls_tr_pred), np.median(seg_tr_nnzs), np.median(duo_tr_nnzs)],\n",
        "         'tr_25th-tile':[np.percentile(cls_tr_pred,25),np.percentile(seg_tr_nnzs,25), np.percentile(duo_tr_nnzs,25)],\n",
        "         'tr_75th-tile':[np.percentile(cls_tr_pred,75),np.percentile(seg_tr_nnzs,75), np.percentile(duo_tr_nnzs,75)]     \n",
        "         }\n",
        "df_train = pd.DataFrame(data_train, index=['model 1', 'model 2', 'model 3'])                             \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "318yi34x7HX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "362989c7-800e-4825-e27b-c14a9611bb94"
      },
      "source": [
        "print(\"Training cohort statistics\")\n",
        "df_train"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training cohort statistics\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tr_mean</th>\n",
              "      <th>tr_median</th>\n",
              "      <th>tr_25th-tile</th>\n",
              "      <th>tr_75th-tile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>model 1</th>\n",
              "      <td>0.876</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model 2</th>\n",
              "      <td>0.886</td>\n",
              "      <td>81.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5044.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model 3</th>\n",
              "      <td>0.867</td>\n",
              "      <td>314.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4145.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         tr_mean  tr_median  tr_25th-tile  tr_75th-tile\n",
              "model 1    0.876        0.0           0.0          1.00\n",
              "model 2    0.886       81.0           0.0       5044.75\n",
              "model 3    0.867      314.0           9.0       4145.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DgPaZyZAJ7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_valid = {'val_mean':[cls_val_mean, seg_val_mean, duo_val_mean],\n",
        "         'val_median':[np.median(cls_val_pred), np.median(seg_val_nnzs), np.median(duo_val_nnzs)],\n",
        "         'val_25th-tile':[np.percentile(cls_val_pred,25),np.percentile(seg_val_nnzs,25), np.percentile(duo_val_nnzs,25)],\n",
        "         'val_75th-tile':[np.percentile(cls_val_pred,75),np.percentile(seg_val_nnzs,75), np.percentile(duo_val_nnzs,75)]     \n",
        "         }\n",
        "df_valid = pd.DataFrame(data_valid, index=['model 1', 'model 2', 'model 3'])   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqpOnW5FAKIy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "848b81af-f447-40db-fc1d-ef2c8e36ec7e"
      },
      "source": [
        "print(\"Validation cohort statistics\")\n",
        "df_valid"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation cohort statistics\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>val_mean</th>\n",
              "      <th>val_median</th>\n",
              "      <th>val_25th-tile</th>\n",
              "      <th>val_75th-tile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>model 1</th>\n",
              "      <td>0.863</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model 2</th>\n",
              "      <td>0.884</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4009.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model 3</th>\n",
              "      <td>0.850</td>\n",
              "      <td>189.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2984.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         val_mean  val_median  val_25th-tile  val_75th-tile\n",
              "model 1     0.863         0.0            0.0            1.0\n",
              "model 2     0.884         0.0            0.0         4009.0\n",
              "model 3     0.850       189.0            7.0         2984.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO7MrYJrEZhZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "808eefbc-ac40-4520-9d86-9d7a436edfd9"
      },
      "source": [
        "result = pd.concat([df_train, df_valid.reindex(df_train.index)], axis=1)\n",
        "result"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tr_mean</th>\n",
              "      <th>tr_median</th>\n",
              "      <th>tr_25th-tile</th>\n",
              "      <th>tr_75th-tile</th>\n",
              "      <th>val_mean</th>\n",
              "      <th>val_median</th>\n",
              "      <th>val_25th-tile</th>\n",
              "      <th>val_75th-tile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>model 1</th>\n",
              "      <td>0.876</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.863</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model 2</th>\n",
              "      <td>0.886</td>\n",
              "      <td>81.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5044.75</td>\n",
              "      <td>0.884</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4009.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model 3</th>\n",
              "      <td>0.867</td>\n",
              "      <td>314.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4145.75</td>\n",
              "      <td>0.850</td>\n",
              "      <td>189.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2984.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         tr_mean  tr_median  ...  val_25th-tile  val_75th-tile\n",
              "model 1    0.876        0.0  ...            0.0            1.0\n",
              "model 2    0.886       81.0  ...            0.0         4009.0\n",
              "model 3    0.867      314.0  ...            7.0         2984.0\n",
              "\n",
              "[3 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU6kNUObEdt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Serialize *.csv\n",
        "fname = '{}/models/final/gej4_results.csv'.format(MOUNT_ROOT)\n",
        "os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
        "result.to_csv(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80SgBLXbJeDz",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In addition to algorithm training as above, a brief write-up is required for this project (minimum of one page). The goal is to *briefly* summarize algorithm design and key results. The write-up should be divided into three sections: methods; results; discussion.\n",
        "\n",
        "### Methods\n",
        "\n",
        "In this section, include details such as:\n",
        "\n",
        "* **Data**: How much data was used. How many cases were utilized for training and validation?\n",
        "* **Network design**: What are the different network architectures? How many layers and parameters? Were 2D or 3D operations used? Recall that the `model.summary(...)` can be used to provide key summary statistics for this purpose. If desired, feel free to include a model figure or diagram.\n",
        "* **Implementation**: How was training implemented. What are the key hyperparameters (e.g. learning rate, batch size, optimizer, etc)? How many training iterations were required for convergence? Did these hyperparameters change during the course of training?\n",
        "* **Statistics**: What statistics do you plan to use to evaluate model accuracy? \n",
        "\n",
        "### Results\n",
        "\n",
        "In this section, briefly summarize experimental results (a few sentences), and include the result table(s) as derived above.\n",
        "\n",
        "### Discussion\n",
        "\n",
        "Were the results expected or unexpected? What accounts for the differences in performance between the algorithms? Which loss types were most effective for this task (e.g. classification, segmentation, etc)? With more time and/or resources, how would further optimize your top model? Feel free to elaborate on any additional observations noted during the course of this expierment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3847aod3JeD0",
        "colab_type": "text"
      },
      "source": [
        "# Submission\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okXInTrAJeD0",
        "colab_type": "text"
      },
      "source": [
        "### Canvas\n",
        "\n",
        "Once you have completed the midterm assignment, download the necessary files from Google Colab and your Google Drive. As in prior assigments, be sure to prepare:\n",
        "\n",
        "* final (completed) notebook: `[UCInetID]_assignment.ipynb`\n",
        "* final (results) spreadsheet: `[UCInetID]_results.csv` (compiled for all three parts)\n",
        "* final (trained) model: `[UCInetID]_model.hdf5` (three separate files for all three parts)\n",
        "\n",
        "In addition, submit the summary write-up as in any common document format (`.docx`, `.tex`, `.pdf`, etc):\n",
        "\n",
        "* final summary write-up: `[UCInetID]_summary.[docx|tex|pdf]`\n",
        "\n",
        "**Important**: please submit all your files prefixed with your UCInetID as listed above. Your UCInetID is the part of your UCI email address that comes before `@uci.edu`. For example, Peter Anteater has an email address of panteater@uci.edu, so his notebooke file would be submitted under the name `panteater_notebook.ipynb`, his spreadsheet would be submitted under the name `panteater_results.csv` and and his model file would be submitted under the name `panteater_model.hdf5`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJKG2ky-N7oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname = '{}/models/final/model_seg.hdf5'.format(MOUNT_ROOT)\n",
        "os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
        "model_seg.save(fname)\n",
        "\n",
        "fname = '{}/models/final/model_cls.hdf5'.format(MOUNT_ROOT)\n",
        "os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
        "model_cls.save(fname)\n",
        "\n",
        "fname = '{}/models/final/model_duo.hdf5'.format(MOUNT_ROOT)\n",
        "os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
        "model_duo.save(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPBVD0n0OQgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnm4aaDxxEq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}