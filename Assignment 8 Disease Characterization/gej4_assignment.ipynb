{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTB5SVdOzHuK",
        "colab_type": "text"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "In this tutorial we create a global classifier to detect pulmonary infection (pneumonia) on chest radiographs. Given the small dataset of 100 training and 100 test exams, strategies to mitigate the effect of CNN overfitting on small size data cohorts will be required. \n",
        "\n",
        "This assignment is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16dXuQXkzHuL",
        "colab_type": "text"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Once complete, the following items must be submitted:\n",
        "\n",
        "* final `*.ipynb` notebook\n",
        "* final trained `*.hdf5` model file\n",
        "* final compiled `*.csv` file with performance statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "56d3oMiMw8Wm"
      },
      "source": [
        "# Google Colab\n",
        "\n",
        "The following lines of code will configure your Google Colab environment for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBHQK7XwzHuM",
        "colab_type": "text"
      },
      "source": [
        "### Enable GPU runtime\n",
        "\n",
        "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
        "\n",
        "```\n",
        "Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poRmgz_JzHuN",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "The Google Colab environment is transient and will reset after any prolonged break in activity. To retain important and/or large files between sessions, use the following lines of code to mount your personal Google drive to this Colab instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JuwKbztzHuO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "41fbe269-ca0a-4b58-ccd1-f2ff945df6b3"
      },
      "source": [
        "try:\n",
        "    # --- Mount gdrive to /content/drive/My Drive/\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "except: pass"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DKRq-V4zHuT",
        "colab_type": "text"
      },
      "source": [
        "Throughout this assignment we will use the following global `MOUNT_ROOT` variable to reference a location to store long-term data. If you are using a local Jupyter server and/or wish to store your data elsewhere, please update this variable now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pq4HV6FzHuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Set data directory\n",
        "MOUNT_ROOT = '/content/drive/My Drive'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqejIsHozHuX",
        "colab_type": "text"
      },
      "source": [
        "### Select Tensorflow library version\n",
        "\n",
        "This assignment will use the (new) Tensorflow 2.0 library. Use the following line of code to select this updated version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USI1GjJezHuY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14af85a0-e465-43ec-ffe7-2572a5a65bbe"
      },
      "source": [
        "# --- Select Tensorflow 2.0 (only in Google Colab)\n",
        "% tensorflow_version 2.x\n",
        "% pip install tensorflow-gpu==2.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/93/c7bca39b23aae45cd2e85ad3871c81eccc63b9c5276e926511e2e5b0879d/tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 31kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.4.1)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 30.2MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.29.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 34.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.18.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (3.2.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (3.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1) (1.0.8)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (46.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.1) (2.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (1.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=aea7628cd173e10e61f43df572ed3e1873aee287195cf53b8c6748a93dd5c9da\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement tensorflow-estimator<2.3.0,>=2.2.0, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS-vofj3zHua",
        "colab_type": "text"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbIVgNJNzHub",
        "colab_type": "text"
      },
      "source": [
        "### Jarvis library\n",
        "\n",
        "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqWsuI2yzHub",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "b06bdb5a-811a-4121-f972-f8a302ee332e"
      },
      "source": [
        "# --- Install jarvis (only in Google Colab or local runtime)\n",
        "% pip install jarvis-md"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jarvis-md\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/72/1f859151f6059f40d469225eb9848fa72c6577beb657e24482996930ba08/jarvis_md-0.0.1a10-py3-none-any.whl (69kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (2.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (1.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (3.2.1)\n",
            "Collecting pyyaml>=5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from jarvis-md) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->jarvis-md) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->jarvis-md) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->jarvis-md) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->jarvis-md) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->jarvis-md) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->jarvis-md) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->jarvis-md) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->jarvis-md) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->jarvis-md) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->jarvis-md) (1.24.3)\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=d7f26979aebe77956ee529227c72f0c62aa6e4614e3a4e7aabc5d25b522e7e0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml, jarvis-md\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed jarvis-md-0.0.1a10 pyyaml-5.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9wWVUvhzHue",
        "colab_type": "text"
      },
      "source": [
        "### Imports\n",
        "\n",
        "Use the following lines to import any additional needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwJyBBj9zHue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np, pandas as pd\n",
        "from tensorflow import losses, optimizers\n",
        "from tensorflow.keras import Input, Model, models, layers, metrics, regularizers\n",
        "from jarvis.train import datasets, custom\n",
        "from jarvis.train.client import Client\n",
        "from jarvis.utils.general import overload, tools as jtools\n",
        "from jarvis.utils.display import imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UBXHqJKzHuh",
        "colab_type": "text"
      },
      "source": [
        "# Data\n",
        "\n",
        "The data used in this tutorial will consist of (frontal projection) chest radiographs from the RSNA / Kaggle pneumonia challenge (https://www.kaggle.com/c/rsna-pneumonia-detection-challenge). The chest radiograph is the standard screening exam of choice to identify and trend changes in lung disease including infection (pneumonia). To simulate the problem of small dataset size, only 100 exams will be used for training (50 normal, 50 positive). A separate 100 exams will be used for independent testing.\n",
        "\n",
        "The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/xr_pna`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUWmcK9rzHui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "741b8a5b-81f9-41d7-99d6-f0447038a6ea"
      },
      "source": [
        "# --- Download dataset\n",
        "datasets.download(name='xr/pna-mul')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2020-06-01 08:51:15 ] [====================] 100.000% : Extracting archive (0000608 / 0000608) "
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'code': '/data/raw/xr_pna', 'data': '/data/raw/xr_pna'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrQDXqFzzHun",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "To account for the small training cohort size, three separate strategies must be employed:\n",
        "\n",
        "1. A **dual loss function** network with both classification and segmentation losses (as demonstrated in the tutorial)\n",
        "\n",
        "2. A reduced final feature map of either (`8 x 8`) or (`4 x 4`) instead of the (`16 x 16`) used in the demonstration\n",
        "\n",
        "3. At least one of the following (or both) regularizer techniques:\n",
        "\n",
        "* dropout\n",
        "* L2 regularization\n",
        "\n",
        "Feel free to further optimize the network architecture to further improve model performance. Recommendings include limiting model size to reduce overfitting, early stopping and mixed L1/L2 regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw0usYC1zHun",
        "colab_type": "text"
      },
      "source": [
        "### Overload the `Client` object\n",
        "\n",
        "*Hint*: Ensure to customize the `arrays['xs']['msk']` object to reflect masked values as shown in the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUdJmJSGzHuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@overload(Client)\n",
        "def preprocess(self, arrays, **kwargs):\n",
        "    \"\"\"\n",
        "    Method to create a custom msk array for class weights and/or masks\n",
        "    \n",
        "    \"\"\"\n",
        "    # --- Create msk\n",
        "    msk = np.zeros(arrays['xs']['dat'].shape)\n",
        "\n",
        "    lng = arrays['xs']['msk']\n",
        "    pna = arrays['ys']['pna-seg']\n",
        "\n",
        "    msk[lng > 0] = 1\n",
        "    msk[pna > 0] = 1\n",
        "\n",
        "    arrays['xs']['msk'] = msk\n",
        "    \n",
        "    return arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLzVFCDrzHuq",
        "colab_type": "text"
      },
      "source": [
        "### Create `Client` object\n",
        "\n",
        "After manually overloading the `Client` object, manually create a new client object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnzPOTRpzHur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Find client yml file\n",
        "yml = '{}/data/ymls/client-mul.yml'.format(jtools.get_paths('xr/pna')['code'])\n",
        "\n",
        "# --- Manually create Client\n",
        "client = Client(yml)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSrSxesezHut",
        "colab_type": "text"
      },
      "source": [
        "### Create inputs and generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKWdlNgJzHut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Manually create generators\n",
        "gen_train, gen_valid = client.create_generators()\n",
        "\n",
        "# --- Create model inputs\n",
        "inputs = client.get_inputs(Input)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t33Tcs3zzHux",
        "colab_type": "text"
      },
      "source": [
        "### Define the model\n",
        "\n",
        "*Hint*: Ensure to use both a classification and segmentation (contracting-encoding) architecture as demonstrated in the tutorial. At this point, also ensure to use either dropout or L2 regularization; other regularization techniques may also be explored to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1w8R1y_zHux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Define kwargs dictionary\n",
        "kwargs = {\n",
        "    'kernel_size': (1, 3, 3),\n",
        "    'padding': 'same',\n",
        "    'kernel_regularizer': regularizers.l2(0.01)}\n",
        "\n",
        "# --- Define lambda functions\n",
        "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.LeakyReLU()(x)\n",
        "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
        "\n",
        "# --- Define stride-1, stride-2 blocks\n",
        "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=(1, 2, 2))))\n",
        "\n",
        "# --- Define dropout\n",
        "drop = layers.Dropout(rate=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj2lIL841G7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Define contracting layers\n",
        "l1 = conv1(8, inputs['dat'])\n",
        "l2 = conv1(16, conv2(16, l1))\n",
        "l3 = conv1(32, conv2(32, l2))\n",
        "l4 = conv1(48, conv2(48, l3))\n",
        "l5 = conv1(64, conv2(64, l4))\n",
        "l6 = conv1(80, conv2(80, l5))\n",
        "l6_conv = drop(conv1(96, conv2(96, l6)))\n",
        "\n",
        "\n",
        "# --- Define expanding layers\n",
        "l6_tran = tran2(80, l6_conv)\n",
        "l7  = tran2(64, conv1(80, l6_tran  + l6))\n",
        "l8  = tran2(48, conv1(64, l7  + l5))\n",
        "l9  = tran2(32, conv1(48, l8  + l4))\n",
        "l10 = tran2(16, conv1(32, l9  + l3))\n",
        "l11 = tran2(8,  conv1(16, l10 + l2))\n",
        "l12 = conv1(8,  conv1(8,  l11 + l1))\n",
        "\n",
        "# --- Create classifier feature vector\n",
        "c1 = layers.Reshape((-1, 1, 1, 8 * 8 * 96))(l6_conv)\n",
        "\n",
        "# --- Create logits\n",
        "logits = {}\n",
        "logits['pna-seg'] = layers.Conv3D(filters=2, name='pna-seg', **kwargs)(l12)\n",
        "logits['pna-cls'] = layers.Conv3D(filters=2, kernel_size=(1, 1, 1), name='pna-cls')(c1)\n",
        "\n",
        "# --- Create model\n",
        "model = Model(inputs=inputs, outputs=logits) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIYsAT5rzHuz",
        "colab_type": "text"
      },
      "source": [
        "### Compile the model\n",
        "\n",
        "*Hint*: Ensure that custom loss functions are used as described in the tutorial to properly mask the segmentation loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWwbFoD0zHu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=2e-4),\n",
        "    loss={\n",
        "        'pna-seg': custom.sce(inputs['msk']),\n",
        "        'pna-cls': losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
        "    metrics={\n",
        "        'pna-seg': custom.dsc(weights=inputs['msk']),\n",
        "        'pna-cls': metrics.SparseCategoricalAccuracy()\n",
        "        },\n",
        "    experimental_run_tf_function=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opyqyoiXzHu4",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Use the following cell block to train your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLfz6eHizHu5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "0fb4270c-0978-4449-a0c5-e682622a8001"
      },
      "source": [
        "# --- Load data into memory for faster training\n",
        "client.load_data_in_memory()\n",
        "\n",
        "# --- Train model\n",
        "model.fit(\n",
        "    x=gen_train, \n",
        "    steps_per_epoch=100, \n",
        "    epochs=6,\n",
        "    validation_data=gen_valid,\n",
        "    validation_steps=100,\n",
        "    validation_freq=2,\n",
        "    use_multiprocessing=True)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[ 2020-06-01 10:39:00 ] [>...................] 0.500% : Iterating | 000001      \r[ 2020-06-01 10:39:00 ] [>...................] 1.000% : Iterating | 000002      \r[ 2020-06-01 10:39:00 ] [>...................] 1.500% : Iterating | 000003      \r[ 2020-06-01 10:39:00 ] [>...................] 2.000% : Iterating | 000004      \r[ 2020-06-01 10:39:00 ] [>...................] 2.500% : Iterating | 000005      \r[ 2020-06-01 10:39:00 ] [>...................] 3.000% : Iterating | 000006      \r[ 2020-06-01 10:39:00 ] [>...................] 3.500% : Iterating | 000007      \r[ 2020-06-01 10:39:00 ] [>...................] 4.000% : Iterating | 000008      \r[ 2020-06-01 10:39:00 ] [>...................] 4.500% : Iterating | 000009      \r[ 2020-06-01 10:39:00 ] [=>..................] 5.000% : Iterating | 000010      \r[ 2020-06-01 10:39:00 ] [=>..................] 5.500% : Iterating | 000011      \r[ 2020-06-01 10:39:00 ] [=>..................] 6.000% : Iterating | 000012      \r[ 2020-06-01 10:39:00 ] [=>..................] 6.500% : Iterating | 000013      \r[ 2020-06-01 10:39:00 ] [=>..................] 7.000% : Iterating | 000014      \r[ 2020-06-01 10:39:00 ] [=>..................] 7.500% : Iterating | 000015      \r[ 2020-06-01 10:39:00 ] [=>..................] 8.000% : Iterating | 000016      \r[ 2020-06-01 10:39:00 ] [=>..................] 8.500% : Iterating | 000017      \r[ 2020-06-01 10:39:00 ] [=>..................] 9.000% : Iterating | 000018      \r[ 2020-06-01 10:39:00 ] [=>..................] 9.500% : Iterating | 000019      \r[ 2020-06-01 10:39:00 ] [==>.................] 10.000% : Iterating | 000020     \r[ 2020-06-01 10:39:00 ] [==>.................] 10.500% : Iterating | 000021     \r[ 2020-06-01 10:39:00 ] [==>.................] 11.000% : Iterating | 000022     \r[ 2020-06-01 10:39:00 ] [==>.................] 11.500% : Iterating | 000023     \r[ 2020-06-01 10:39:00 ] [==>.................] 12.000% : Iterating | 000024     \r[ 2020-06-01 10:39:00 ] [==>.................] 12.500% : Iterating | 000025     \r[ 2020-06-01 10:39:00 ] [==>.................] 13.000% : Iterating | 000026     \r[ 2020-06-01 10:39:00 ] [==>.................] 13.500% : Iterating | 000027     \r[ 2020-06-01 10:39:00 ] [==>.................] 14.000% : Iterating | 000028     \r[ 2020-06-01 10:39:00 ] [==>.................] 14.500% : Iterating | 000029     \r[ 2020-06-01 10:39:00 ] [===>................] 15.000% : Iterating | 000030     \r[ 2020-06-01 10:39:00 ] [===>................] 15.500% : Iterating | 000031     \r[ 2020-06-01 10:39:00 ] [===>................] 16.000% : Iterating | 000032     \r[ 2020-06-01 10:39:00 ] [===>................] 16.500% : Iterating | 000033     \r[ 2020-06-01 10:39:00 ] [===>................] 17.000% : Iterating | 000034     \r[ 2020-06-01 10:39:00 ] [===>................] 17.500% : Iterating | 000035     \r[ 2020-06-01 10:39:00 ] [===>................] 18.000% : Iterating | 000036     \r[ 2020-06-01 10:39:00 ] [===>................] 18.500% : Iterating | 000037     \r[ 2020-06-01 10:39:00 ] [===>................] 19.000% : Iterating | 000038     \r[ 2020-06-01 10:39:00 ] [===>................] 19.500% : Iterating | 000039     \r[ 2020-06-01 10:39:00 ] [====>...............] 20.000% : Iterating | 000040     \r[ 2020-06-01 10:39:00 ] [====>...............] 20.500% : Iterating | 000041     \r[ 2020-06-01 10:39:00 ] [====>...............] 21.000% : Iterating | 000042     \r[ 2020-06-01 10:39:00 ] [====>...............] 21.500% : Iterating | 000043     \r[ 2020-06-01 10:39:00 ] [====>...............] 22.000% : Iterating | 000044     \r[ 2020-06-01 10:39:00 ] [====>...............] 22.500% : Iterating | 000045     \r[ 2020-06-01 10:39:00 ] [====>...............] 23.000% : Iterating | 000046     \r[ 2020-06-01 10:39:00 ] [====>...............] 23.500% : Iterating | 000047     \r[ 2020-06-01 10:39:00 ] [====>...............] 24.000% : Iterating | 000048     \r[ 2020-06-01 10:39:00 ] [====>...............] 24.500% : Iterating | 000049     \r[ 2020-06-01 10:39:00 ] [=====>..............] 25.000% : Iterating | 000050     \r[ 2020-06-01 10:39:00 ] [=====>..............] 25.500% : Iterating | 000051     \r[ 2020-06-01 10:39:00 ] [=====>..............] 26.000% : Iterating | 000052     \r[ 2020-06-01 10:39:00 ] [=====>..............] 26.500% : Iterating | 000053     \r[ 2020-06-01 10:39:00 ] [=====>..............] 27.000% : Iterating | 000054     \r[ 2020-06-01 10:39:00 ] [=====>..............] 27.500% : Iterating | 000055     \r[ 2020-06-01 10:39:00 ] [=====>..............] 28.000% : Iterating | 000056     \r[ 2020-06-01 10:39:00 ] [=====>..............] 28.500% : Iterating | 000057     \r[ 2020-06-01 10:39:00 ] [=====>..............] 29.000% : Iterating | 000058     \r[ 2020-06-01 10:39:00 ] [=====>..............] 29.500% : Iterating | 000059     \r[ 2020-06-01 10:39:00 ] [======>.............] 30.000% : Iterating | 000060     \r[ 2020-06-01 10:39:00 ] [======>.............] 30.500% : Iterating | 000061     \r[ 2020-06-01 10:39:00 ] [======>.............] 31.000% : Iterating | 000062     \r[ 2020-06-01 10:39:00 ] [======>.............] 31.500% : Iterating | 000063     \r[ 2020-06-01 10:39:00 ] [======>.............] 32.000% : Iterating | 000064     \r[ 2020-06-01 10:39:00 ] [======>.............] 32.500% : Iterating | 000065     \r[ 2020-06-01 10:39:00 ] [======>.............] 33.000% : Iterating | 000066     \r[ 2020-06-01 10:39:00 ] [======>.............] 33.500% : Iterating | 000067     \r[ 2020-06-01 10:39:00 ] [======>.............] 34.000% : Iterating | 000068     \r[ 2020-06-01 10:39:00 ] [======>.............] 34.500% : Iterating | 000069     \r[ 2020-06-01 10:39:00 ] [=======>............] 35.000% : Iterating | 000070     \r[ 2020-06-01 10:39:00 ] [=======>............] 35.500% : Iterating | 000071     \r[ 2020-06-01 10:39:00 ] [=======>............] 36.000% : Iterating | 000072     \r[ 2020-06-01 10:39:00 ] [=======>............] 36.500% : Iterating | 000073     \r[ 2020-06-01 10:39:00 ] [=======>............] 37.000% : Iterating | 000074     \r[ 2020-06-01 10:39:00 ] [=======>............] 37.500% : Iterating | 000075     \r[ 2020-06-01 10:39:00 ] [=======>............] 38.000% : Iterating | 000076     \r[ 2020-06-01 10:39:00 ] [=======>............] 38.500% : Iterating | 000077     \r[ 2020-06-01 10:39:00 ] [=======>............] 39.000% : Iterating | 000078     \r[ 2020-06-01 10:39:00 ] [=======>............] 39.500% : Iterating | 000079     \r[ 2020-06-01 10:39:00 ] [========>...........] 40.000% : Iterating | 000080     \r[ 2020-06-01 10:39:00 ] [========>...........] 40.500% : Iterating | 000081     \r[ 2020-06-01 10:39:00 ] [========>...........] 41.000% : Iterating | 000082     \r[ 2020-06-01 10:39:00 ] [========>...........] 41.500% : Iterating | 000083     \r[ 2020-06-01 10:39:00 ] [========>...........] 42.000% : Iterating | 000084     \r[ 2020-06-01 10:39:00 ] [========>...........] 42.500% : Iterating | 000085     \r[ 2020-06-01 10:39:00 ] [========>...........] 43.000% : Iterating | 000086     \r[ 2020-06-01 10:39:00 ] [========>...........] 43.500% : Iterating | 000087     \r[ 2020-06-01 10:39:00 ] [========>...........] 44.000% : Iterating | 000088     \r[ 2020-06-01 10:39:00 ] [========>...........] 44.500% : Iterating | 000089     \r[ 2020-06-01 10:39:00 ] [=========>..........] 45.000% : Iterating | 000090     \r[ 2020-06-01 10:39:00 ] [=========>..........] 45.500% : Iterating | 000091     \r[ 2020-06-01 10:39:00 ] [=========>..........] 46.000% : Iterating | 000092     \r[ 2020-06-01 10:39:00 ] [=========>..........] 46.500% : Iterating | 000093     \r[ 2020-06-01 10:39:00 ] [=========>..........] 47.000% : Iterating | 000094     \r[ 2020-06-01 10:39:00 ] [=========>..........] 47.500% : Iterating | 000095     \r[ 2020-06-01 10:39:00 ] [=========>..........] 48.000% : Iterating | 000096     \r[ 2020-06-01 10:39:00 ] [=========>..........] 48.500% : Iterating | 000097     \r[ 2020-06-01 10:39:00 ] [=========>..........] 49.000% : Iterating | 000098     \r[ 2020-06-01 10:39:00 ] [=========>..........] 49.500% : Iterating | 000099     \r[ 2020-06-01 10:39:00 ] [==========>.........] 50.000% : Iterating | 000100     \r[ 2020-06-01 10:39:00 ] [==========>.........] 50.500% : Iterating | 000101     \r[ 2020-06-01 10:39:00 ] [==========>.........] 51.000% : Iterating | 000102     \r[ 2020-06-01 10:39:00 ] [==========>.........] 51.500% : Iterating | 000103     \r[ 2020-06-01 10:39:00 ] [==========>.........] 52.000% : Iterating | 000104     \r[ 2020-06-01 10:39:00 ] [==========>.........] 52.500% : Iterating | 000105     \r[ 2020-06-01 10:39:00 ] [==========>.........] 53.000% : Iterating | 000106     \r[ 2020-06-01 10:39:00 ] [==========>.........] 53.500% : Iterating | 000107     \r[ 2020-06-01 10:39:00 ] [==========>.........] 54.000% : Iterating | 000108     \r[ 2020-06-01 10:39:00 ] [==========>.........] 54.500% : Iterating | 000109     \r[ 2020-06-01 10:39:00 ] [===========>........] 55.000% : Iterating | 000110     \r[ 2020-06-01 10:39:00 ] [===========>........] 55.500% : Iterating | 000111     \r[ 2020-06-01 10:39:00 ] [===========>........] 56.000% : Iterating | 000112     \r[ 2020-06-01 10:39:00 ] [===========>........] 56.500% : Iterating | 000113     \r[ 2020-06-01 10:39:00 ] [===========>........] 57.000% : Iterating | 000114     \r[ 2020-06-01 10:39:00 ] [===========>........] 57.500% : Iterating | 000115     \r[ 2020-06-01 10:39:00 ] [===========>........] 58.000% : Iterating | 000116     \r[ 2020-06-01 10:39:00 ] [===========>........] 58.500% : Iterating | 000117     \r[ 2020-06-01 10:39:00 ] [===========>........] 59.000% : Iterating | 000118     \r[ 2020-06-01 10:39:00 ] [===========>........] 59.500% : Iterating | 000119     \r[ 2020-06-01 10:39:00 ] [============>.......] 60.000% : Iterating | 000120     \r[ 2020-06-01 10:39:00 ] [============>.......] 60.500% : Iterating | 000121     \r[ 2020-06-01 10:39:00 ] [============>.......] 61.000% : Iterating | 000122     \r[ 2020-06-01 10:39:00 ] [============>.......] 61.500% : Iterating | 000123     \r[ 2020-06-01 10:39:00 ] [============>.......] 62.000% : Iterating | 000124     \r[ 2020-06-01 10:39:00 ] [============>.......] 62.500% : Iterating | 000125     \r[ 2020-06-01 10:39:00 ] [============>.......] 63.000% : Iterating | 000126     \r[ 2020-06-01 10:39:00 ] [============>.......] 63.500% : Iterating | 000127     \r[ 2020-06-01 10:39:00 ] [============>.......] 64.000% : Iterating | 000128     \r[ 2020-06-01 10:39:00 ] [============>.......] 64.500% : Iterating | 000129     \r[ 2020-06-01 10:39:00 ] [=============>......] 65.000% : Iterating | 000130     \r[ 2020-06-01 10:39:00 ] [=============>......] 65.500% : Iterating | 000131     \r[ 2020-06-01 10:39:00 ] [=============>......] 66.000% : Iterating | 000132     \r[ 2020-06-01 10:39:00 ] [=============>......] 66.500% : Iterating | 000133     \r[ 2020-06-01 10:39:00 ] [=============>......] 67.000% : Iterating | 000134     \r[ 2020-06-01 10:39:00 ] [=============>......] 67.500% : Iterating | 000135     \r[ 2020-06-01 10:39:00 ] [=============>......] 68.000% : Iterating | 000136     \r[ 2020-06-01 10:39:00 ] [=============>......] 68.500% : Iterating | 000137     \r[ 2020-06-01 10:39:00 ] [=============>......] 69.000% : Iterating | 000138     \r[ 2020-06-01 10:39:00 ] [=============>......] 69.500% : Iterating | 000139     \r[ 2020-06-01 10:39:00 ] [==============>.....] 70.000% : Iterating | 000140     \r[ 2020-06-01 10:39:00 ] [==============>.....] 70.500% : Iterating | 000141     \r[ 2020-06-01 10:39:00 ] [==============>.....] 71.000% : Iterating | 000142     \r[ 2020-06-01 10:39:00 ] [==============>.....] 71.500% : Iterating | 000143     \r[ 2020-06-01 10:39:00 ] [==============>.....] 72.000% : Iterating | 000144     \r[ 2020-06-01 10:39:00 ] [==============>.....] 72.500% : Iterating | 000145     \r[ 2020-06-01 10:39:00 ] [==============>.....] 73.000% : Iterating | 000146     \r[ 2020-06-01 10:39:00 ] [==============>.....] 73.500% : Iterating | 000147     \r[ 2020-06-01 10:39:00 ] [==============>.....] 74.000% : Iterating | 000148     \r[ 2020-06-01 10:39:00 ] [==============>.....] 74.500% : Iterating | 000149     \r[ 2020-06-01 10:39:00 ] [===============>....] 75.000% : Iterating | 000150     \r[ 2020-06-01 10:39:00 ] [===============>....] 75.500% : Iterating | 000151     \r[ 2020-06-01 10:39:00 ] [===============>....] 76.000% : Iterating | 000152     \r[ 2020-06-01 10:39:00 ] [===============>....] 76.500% : Iterating | 000153     \r[ 2020-06-01 10:39:00 ] [===============>....] 77.000% : Iterating | 000154     \r[ 2020-06-01 10:39:00 ] [===============>....] 77.500% : Iterating | 000155     \r[ 2020-06-01 10:39:00 ] [===============>....] 78.000% : Iterating | 000156     \r[ 2020-06-01 10:39:00 ] [===============>....] 78.500% : Iterating | 000157     \r[ 2020-06-01 10:39:00 ] [===============>....] 79.000% : Iterating | 000158     \r[ 2020-06-01 10:39:00 ] [===============>....] 79.500% : Iterating | 000159     \r[ 2020-06-01 10:39:00 ] [================>...] 80.000% : Iterating | 000160     \r[ 2020-06-01 10:39:00 ] [================>...] 80.500% : Iterating | 000161     \r[ 2020-06-01 10:39:00 ] [================>...] 81.000% : Iterating | 000162     \r[ 2020-06-01 10:39:00 ] [================>...] 81.500% : Iterating | 000163     \r[ 2020-06-01 10:39:00 ] [================>...] 82.000% : Iterating | 000164     \r[ 2020-06-01 10:39:00 ] [================>...] 82.500% : Iterating | 000165     \r[ 2020-06-01 10:39:00 ] [================>...] 83.000% : Iterating | 000166     \r[ 2020-06-01 10:39:00 ] [================>...] 83.500% : Iterating | 000167     \r[ 2020-06-01 10:39:00 ] [================>...] 84.000% : Iterating | 000168     \r[ 2020-06-01 10:39:00 ] [================>...] 84.500% : Iterating | 000169     \r[ 2020-06-01 10:39:00 ] [=================>..] 85.000% : Iterating | 000170     \r[ 2020-06-01 10:39:00 ] [=================>..] 85.500% : Iterating | 000171     \r[ 2020-06-01 10:39:00 ] [=================>..] 86.000% : Iterating | 000172     \r[ 2020-06-01 10:39:00 ] [=================>..] 86.500% : Iterating | 000173     \r[ 2020-06-01 10:39:00 ] [=================>..] 87.000% : Iterating | 000174     \r[ 2020-06-01 10:39:00 ] [=================>..] 87.500% : Iterating | 000175     \r[ 2020-06-01 10:39:00 ] [=================>..] 88.000% : Iterating | 000176     \r[ 2020-06-01 10:39:00 ] [=================>..] 88.500% : Iterating | 000177     \r[ 2020-06-01 10:39:00 ] [=================>..] 89.000% : Iterating | 000178     \r[ 2020-06-01 10:39:00 ] [=================>..] 89.500% : Iterating | 000179     \r[ 2020-06-01 10:39:00 ] [==================>.] 90.000% : Iterating | 000180     \r[ 2020-06-01 10:39:00 ] [==================>.] 90.500% : Iterating | 000181     \r[ 2020-06-01 10:39:00 ] [==================>.] 91.000% : Iterating | 000182     \r[ 2020-06-01 10:39:00 ] [==================>.] 91.500% : Iterating | 000183     \r[ 2020-06-01 10:39:00 ] [==================>.] 92.000% : Iterating | 000184     \r[ 2020-06-01 10:39:00 ] [==================>.] 92.500% : Iterating | 000185     \r[ 2020-06-01 10:39:00 ] [==================>.] 93.000% : Iterating | 000186     \r[ 2020-06-01 10:39:00 ] [==================>.] 93.500% : Iterating | 000187     \r[ 2020-06-01 10:39:00 ] [==================>.] 94.000% : Iterating | 000188     \r[ 2020-06-01 10:39:00 ] [==================>.] 94.500% : Iterating | 000189     \r[ 2020-06-01 10:39:00 ] [===================>] 95.000% : Iterating | 000190     \r[ 2020-06-01 10:39:00 ] [===================>] 95.500% : Iterating | 000191     \r[ 2020-06-01 10:39:00 ] [===================>] 96.000% : Iterating | 000192     \r[ 2020-06-01 10:39:00 ] [===================>] 96.500% : Iterating | 000193     \r[ 2020-06-01 10:39:00 ] [===================>] 97.000% : Iterating | 000194     \r[ 2020-06-01 10:39:00 ] [===================>] 97.500% : Iterating | 000195     \r[ 2020-06-01 10:39:00 ] [===================>] 98.000% : Iterating | 000196     \r[ 2020-06-01 10:39:00 ] [===================>] 98.500% : Iterating | 000197     \r[ 2020-06-01 10:39:00 ] [===================>] 99.000% : Iterating | 000198     \r[ 2020-06-01 10:39:00 ] [===================>] 99.500% : Iterating | 000199     \r[ 2020-06-01 10:39:00 ] [====================] 100.000% : Iterating | 000200    WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 86s 858ms/step - loss: 10.2381 - pna-cls_loss: 0.1439 - pna-seg_loss: 0.1490 - pna-cls_sparse_categorical_accuracy: 0.9400 - pna-seg_dsc_1: 0.1837\n",
            "Epoch 2/6\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 7.7180 - pna-cls_loss: 0.0025 - pna-seg_loss: 0.0872 - pna-cls_sparse_categorical_accuracy: 1.0000 - pna-seg_dsc_1: 0.3133WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 26s 265ms/step - loss: 7.5590 - pna-cls_loss: 0.5644 - pna-seg_loss: 0.1211 - pna-cls_sparse_categorical_accuracy: 0.6313 - pna-seg_dsc_1: 0.0013\n",
            "100/100 [==============================] - 100s 1s/step - loss: 7.7104 - pna-cls_loss: 0.0025 - pna-seg_loss: 0.0870 - pna-cls_sparse_categorical_accuracy: 1.0000 - pna-seg_dsc_1: 0.3157 - val_loss: 7.5590 - val_pna-cls_loss: 0.5644 - val_pna-seg_loss: 0.1211 - val_pna-cls_sparse_categorical_accuracy: 0.6313 - val_pna-seg_dsc_1: 0.0013\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 74s 736ms/step - loss: 6.4103 - pna-cls_loss: 0.0016 - pna-seg_loss: 0.0425 - pna-cls_sparse_categorical_accuracy: 1.0000 - pna-seg_dsc_1: 0.7145\n",
            "Epoch 4/6\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 5.5793 - pna-cls_loss: 8.1792e-04 - pna-seg_loss: 0.0183 - pna-cls_sparse_categorical_accuracy: 1.0000 - pna-seg_dsc_1: 0.8874WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 24s 238ms/step - loss: 5.6797 - pna-cls_loss: 0.3422 - pna-seg_loss: 0.1213 - pna-cls_sparse_categorical_accuracy: 0.8537 - pna-seg_dsc_1: 0.0011\n",
            "100/100 [==============================] - 97s 973ms/step - loss: 5.5760 - pna-cls_loss: 8.1617e-04 - pna-seg_loss: 0.0184 - pna-cls_sparse_categorical_accuracy: 1.0000 - pna-seg_dsc_1: 0.8874 - val_loss: 5.6797 - val_pna-cls_loss: 0.3422 - val_pna-seg_loss: 0.1213 - val_pna-cls_sparse_categorical_accuracy: 0.8537 - val_pna-seg_dsc_1: 0.0011\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 74s 736ms/step - loss: 4.9379 - pna-cls_loss: 5.9426e-04 - pna-seg_loss: 0.0158 - pna-cls_sparse_categorical_accuracy: 1.0000 - pna-seg_dsc_1: 0.8914\n",
            "Epoch 6/6\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.3821 - pna-cls_loss: 5.1213e-04 - pna-seg_loss: 0.0114 - pna-cls_sparse_categorical_accuracy: 1.0000 - pna-seg_dsc_1: 0.9200WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 24s 236ms/step - loss: 4.5325 - pna-cls_loss: 0.3114 - pna-seg_loss: 0.1148 - pna-cls_sparse_categorical_accuracy: 0.8700 - pna-seg_dsc_1: 0.0076\n",
            "100/100 [==============================] - 97s 971ms/step - loss: 4.3795 - pna-cls_loss: 5.1039e-04 - pna-seg_loss: 0.0114 - pna-cls_sparse_categorical_accuracy: 1.0000 - pna-seg_dsc_1: 0.9204 - val_loss: 4.5325 - val_pna-cls_loss: 0.3114 - val_pna-seg_loss: 0.1148 - val_pna-cls_sparse_categorical_accuracy: 0.8700 - val_pna-seg_dsc_1: 0.0076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efde3d67828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dMqaVyDzHu8",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Based on the tutorial discussion, use the following cells to calculate model performance. Only model accuracy (global classification performance) needs to evaluated (Dice score does not).\n",
        "\n",
        "### Performance\n",
        "\n",
        "The following minimum performance metrics must be met for full credit:\n",
        "\n",
        "* accuracy: >0.82"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6HZQs8wzHu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY2zu0M_zHu_",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "\n",
        "When ready, create a `*.csv` file with your compiled **validation** cohort accuracy statistics. There is no need to submit training performance accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfIMCADmzHu_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c142573-c026-41b8-d6bc-a716a359bc18"
      },
      "source": [
        "accuracy = []\n",
        "\n",
        "for x, y in test_valid:\n",
        "    \n",
        "    # --- Create prediction\n",
        "    logits = model.predict(x)\n",
        "    pred = np.argmax(logits[0], axis=-1)\n",
        "    \n",
        "    # --- Compare with ground truth\n",
        "    accuracy.append(pred.squeeze() == y['pna-cls'].squeeze())\n",
        "\n",
        "accuracy = np.array(accuracy)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2020-06-01 10:47:59 ] [====================] 100.000% : Iterating | 000100    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpX120N8QQY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Define columns\n",
        "df = pd.DataFrame(index=np.arange(accuracy.size))\n",
        "df['accuracy'] = accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAQt1UazQlrk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee029015-a77d-4096-d15f-8123509bc8f9"
      },
      "source": [
        "print (df['accuracy'].mean())"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a66h8hrYgCOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# --- Serialize *.csv\n",
        "fname = '{}/models/disease_characterization/results.csv'.format(MOUNT_ROOT)\n",
        "os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
        "df.to_csv(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGrF7f5KzHvC",
        "colab_type": "text"
      },
      "source": [
        "# Submission\n",
        "\n",
        "Use the following line to save your model for submission (in Google Colab this should save your model file into your personal Google Drive):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anOlDqOMzHvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Serialize a model\n",
        "fname = '{}/models/disease_characterization/final.hdf5'.format(MOUNT_ROOT)\n",
        "os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
        "model.save(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ClQlaRzHvE",
        "colab_type": "text"
      },
      "source": [
        "### Canvas\n",
        "\n",
        "Once you have completed this assignment, download the necessary files from Google Colab and your Google Drive. You will then need to submit the following items:\n",
        "\n",
        "* final (completed) notebook: `[UCInetID]_assignment.ipynb`\n",
        "* final (results) spreadsheet: `[UCInetID]_results.csv`\n",
        "* final (trained) model: `[UCInetID]_model.hdf5`\n",
        "\n",
        "**Important**: please submit all your files prefixed with your UCInetID as listed above. Your UCInetID is the part of your UCI email address that comes before `@uci.edu`. For example, Peter Anteater has an email address of panteater@uci.edu, so his notebooke file would be submitted under the name `panteater_notebook.ipynb`, his spreadshhet would be submitted under the name `panteater_results.csv` and and his model file would be submitted under the name `panteater_model.hdf5`."
      ]
    }
  ]
}